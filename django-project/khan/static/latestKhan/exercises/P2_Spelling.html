<!DOCTYPE html>
<html data-require="word-problems subhints">
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Spelling</title>
    <script src="../khan-exercise.js"></script>
</head>

<body>
    <div class="exercise">
        <div class="problems">
            <div id="p1">
                <div class="question">
                	<div class="definition" id="nc">
						In a noisy channel model, you're trying to find the intended word given a word where the letters have been scrambled in some manner.
					</div> <!--definition-->
					<p>Suppose we want to smooth the likelihood term of a <a href="#" class="show-definition" data-definition="nc">noisy
					channel</a>
					model of spelling. We are given two words, <code>x</code> and <code>w</code>, where
					<code>x</code> is the same as <code>w</code>, except the letter <code>w_{i-1}</code> in <code>w</code> has
					been miss typed as <code>w_{i-1}x_i</code> in <code>x</code>.

					Specifically, we want to apply add-one smoothing to <code>P(x|w)</code>, the
					probability of typing <code>w_{i-1}x_i</code> instead of <code>w_{i-1}</code>, where
					<code>x_i</code> and <code>w_{i-1}</code> are single letters.

					For insertions, <code>P(x|w) = \frac{ins[w_{i-1}, x_i]}{c(w_{i-1})}</code>,
					where <code>ins[w_{i-1}, x_i]</code> is the number of times that <code>x_i</code> is
					inserted after <code>w_{i-1}</code> in the corpus, and <code>c(w_{i-1})</code> is the
					number of times letter <code>w_{i-1}</code> appears in our corpus. Again,
					please note that here <code>x_i</code> and <code>w_{i-1}</code> are individual letters,
					not words. <p> What is the formula for <code>P(x|w)</code> if we use add-one
					smoothing to the insertion edit model? Assume the only characters we
					use are lowercase a-z, that there are <code>V</code> word types in our corpus,
					and <code>n</code> total characters, not counting spaces. </p>
                </div> <!--question-->
                
                <div class="solution"><code>\Large \frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + 26}</code></div>
                <ul class="choices">
                    <li><code>\Large \frac{ins[w_{i-1}, x_i]}{c(w_{i-1})}</code></li>
                    <li><code>\Large \frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + V}</code></li>
                    <li><code>\Large \frac{ins[w_{i-1}, x_i] + 1}{c(w_{i-1}) + n}</code></li>
                </ul>
                
     			
                <div class="definition" id="pxw">
                	The probability of x, given w
                </div>
     		    <div class="hints">
                <p>
                	The number of times we add one is independent of the vocabulary size.
                </p>
                <p>
                	We compute <a href="#" class="show-definition" data-definition="pxw">P(x|w)</a> for each letter, not for each occurrence of a letter
                	in the corpus. Thus, adding n to the denominator isn't correct.
                </p>
                <p>
					The distribution P(x|w) has <code class="hint_blue">26</code> entries, one for
					each possible value of x_i. Thus, we add <code class="hint_blue">26</code>
					total fictional counts to our data, which means we must add
					<code class="hint_blue">26</code> to the denominator.
                </p>
                </div> <!--hint-->
        	</div> <!--prob1-->
                
		</div> <!--problems-->
	</div> <!--exercise-->
    
    
</body>
</html>
